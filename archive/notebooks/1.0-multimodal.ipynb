{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIMODAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ce notebook permet d'entraîner le modèle Clip d'Open AI, fait pour associer texte et image et effectuer des tâches comme la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(text):\n",
    "    with open(\"logs/debug_log.txt\", \"a\") as f:\n",
    "        sys.stdout = f\n",
    "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"[{current_time}] : {text}\")\n",
    "        sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Définition des variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/processed/img_classified_by_prdtypecode/img_classified_by_prdtypecode\"\n",
    "CSV_FILE = \"../data/csv_files/img-text-clean-data.csv\"\n",
    "SAVE_DIR = \"save\"\n",
    "ACC_LOSS_HIST_DIR = \"acc_loss_history\"\n",
    "NUM_CLASSES = 27\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "LR_REDUCER_PATIENCE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names :\n",
      " ['10', '1140', '1160', '1180', '1280', '1281', '1300', '1301', '1302', '1320', '1560', '1920', '1940', '2060', '2220', '2280', '2403', '2462', '2522', '2582', '2583', '2585', '2705', '2905', '40', '50', '60']\n",
      "label_to_index :\n",
      " {'10': 0, '1140': 1, '1160': 2, '1180': 3, '1280': 4, '1281': 5, '1300': 6, '1301': 7, '1302': 8, '1320': 9, '1560': 10, '1920': 11, '1940': 12, '2060': 13, '2220': 14, '2280': 15, '2403': 16, '2462': 17, '2522': 18, '2582': 19, '2583': 20, '2585': 21, '2705': 22, '2905': 23, '40': 24, '50': 25, '60': 26}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>image name</th>\n",
       "      <th>image size in bits</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>désignation textuelle</th>\n",
       "      <th>catégorie niv 1</th>\n",
       "      <th>desi_desc_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>14010</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres anciens / occasion</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>olivia personalisiertes notizbuch seiten punkt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>14854</td>\n",
       "      <td>2280</td>\n",
       "      <td>journaux, revues, magazines anciens</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>journal arts art marche salon art asiatique pa...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>6898</td>\n",
       "      <td>50</td>\n",
       "      <td>Accessoires &amp; produits dérivés gaming</td>\n",
       "      <td>Jeux Vidéos</td>\n",
       "      <td>stylet ergonomique bleu gamepad nintendo wii s...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>image_457047496_product_50418756.jpg</td>\n",
       "      <td>14404</td>\n",
       "      <td>1280</td>\n",
       "      <td>Jeux &amp; jouets pour enfants</td>\n",
       "      <td>Jeux de société &amp; Jouets</td>\n",
       "      <td>peluche donald europe disneyland marionnette d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>image_1077757786_product_278535884.jpg</td>\n",
       "      <td>20435</td>\n",
       "      <td>2705</td>\n",
       "      <td>Livres neufs</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>guerre tuques luc ideacute grandeur veut organ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "                                image name  image size in bits  prdtypecode  \\\n",
       "0  image_1263597046_product_3804725264.jpg               14010           10   \n",
       "1   image_1008141237_product_436067568.jpg               14854         2280   \n",
       "2    image_938777978_product_201115110.jpg                6898           50   \n",
       "3     image_457047496_product_50418756.jpg               14404         1280   \n",
       "4   image_1077757786_product_278535884.jpg               20435         2705   \n",
       "\n",
       "                   désignation textuelle           catégorie niv 1  \\\n",
       "0              Livres anciens / occasion               Littérature   \n",
       "1    journaux, revues, magazines anciens               Littérature   \n",
       "2  Accessoires & produits dérivés gaming               Jeux Vidéos   \n",
       "3             Jeux & jouets pour enfants  Jeux de société & Jouets   \n",
       "4                           Livres neufs               Littérature   \n",
       "\n",
       "                                   desi_desc_cleaned  label  \n",
       "0  olivia personalisiertes notizbuch seiten punkt...      0  \n",
       "1  journal arts art marche salon art asiatique pa...     15  \n",
       "2  stylet ergonomique bleu gamepad nintendo wii s...     25  \n",
       "3  peluche donald europe disneyland marionnette d...      4  \n",
       "4  guerre tuques luc ideacute grandeur veut organ...     22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84916 entries, 0 to 84915\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   designation            84916 non-null  object\n",
      " 1   description            55116 non-null  object\n",
      " 2   productid              84916 non-null  int64 \n",
      " 3   imageid                84916 non-null  int64 \n",
      " 4   image name             84916 non-null  object\n",
      " 5   image size in bits     84916 non-null  int64 \n",
      " 6   prdtypecode            84916 non-null  int64 \n",
      " 7   désignation textuelle  84916 non-null  object\n",
      " 8   catégorie niv 1        84916 non-null  object\n",
      " 9   desi_desc_cleaned      84521 non-null  object\n",
      " 10  label                  84916 non-null  Int64 \n",
      "dtypes: Int64(1), int64(4), object(6)\n",
      "memory usage: 7.2+ MB\n",
      "None\n",
      "Valeurs manquantes par colonne :\n",
      " designation                  0\n",
      "description              29800\n",
      "productid                    0\n",
      "imageid                      0\n",
      "image name                   0\n",
      "image size in bits           0\n",
      "prdtypecode                  0\n",
      "désignation textuelle        0\n",
      "catégorie niv 1              0\n",
      "desi_desc_cleaned          395\n",
      "label                        0\n",
      "dtype: int64\n",
      "Lignes avec des valeurs nulles dans 'desi_desc_cleaned' :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>image name</th>\n",
       "      <th>image size in bits</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>désignation textuelle</th>\n",
       "      <th>catégorie niv 1</th>\n",
       "      <th>desi_desc_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Innercity Burnout [Import Allemand] [Jeu Pc]</td>\n",
       "      <td>&lt;br&gt;Attention !!! Ce produit est un import  si...</td>\n",
       "      <td>190095382</td>\n",
       "      <td>933239989</td>\n",
       "      <td>image_933239989_product_190095382.jpg</td>\n",
       "      <td>43805</td>\n",
       "      <td>40</td>\n",
       "      <td>Jeux vidéos anciens, équipement</td>\n",
       "      <td>Jeux Vidéos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Zumba Fitness : Rush (Jeu Kinect) [Import Alle...</td>\n",
       "      <td>&lt;br&gt;Attention !!! Ce produit est un import  si...</td>\n",
       "      <td>190091299</td>\n",
       "      <td>933233675</td>\n",
       "      <td>image_933233675_product_190091299.jpg</td>\n",
       "      <td>46184</td>\n",
       "      <td>40</td>\n",
       "      <td>Jeux vidéos anciens, équipement</td>\n",
       "      <td>Jeux Vidéos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>C/SEP ARRIERE GHE SUPERCHIEF B737117 AD.IHF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1657064547</td>\n",
       "      <td>1096608932</td>\n",
       "      <td>image_1096608932_product_1657064547.jpg</td>\n",
       "      <td>17070</td>\n",
       "      <td>2585</td>\n",
       "      <td>Outillage intérieur / extérieur, tâches ménagères</td>\n",
       "      <td>Maison &amp; ameublement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Assassin's Creed : Brotherhood + Assassin's Cr...</td>\n",
       "      <td>&lt;br&gt;Attention !!! Ce produit est un import  si...</td>\n",
       "      <td>220267350</td>\n",
       "      <td>955658863</td>\n",
       "      <td>image_955658863_product_220267350.jpg</td>\n",
       "      <td>56560</td>\n",
       "      <td>40</td>\n",
       "      <td>Jeux vidéos anciens, équipement</td>\n",
       "      <td>Jeux Vidéos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Planet 51 [Import Allemand] [Jeu Wii]</td>\n",
       "      <td>&lt;br&gt;Attention !!! Ce produit est un import  si...</td>\n",
       "      <td>190093955</td>\n",
       "      <td>933235320</td>\n",
       "      <td>image_933235320_product_190093955.jpg</td>\n",
       "      <td>44208</td>\n",
       "      <td>40</td>\n",
       "      <td>Jeux vidéos anciens, équipement</td>\n",
       "      <td>Jeux Vidéos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           designation  \\\n",
       "101       Innercity Burnout [Import Allemand] [Jeu Pc]   \n",
       "107  Zumba Fitness : Rush (Jeu Kinect) [Import Alle...   \n",
       "339        C/SEP ARRIERE GHE SUPERCHIEF B737117 AD.IHF   \n",
       "346  Assassin's Creed : Brotherhood + Assassin's Cr...   \n",
       "421              Planet 51 [Import Allemand] [Jeu Wii]   \n",
       "\n",
       "                                           description   productid  \\\n",
       "101  <br>Attention !!! Ce produit est un import  si...   190095382   \n",
       "107  <br>Attention !!! Ce produit est un import  si...   190091299   \n",
       "339                                                NaN  1657064547   \n",
       "346  <br>Attention !!! Ce produit est un import  si...   220267350   \n",
       "421  <br>Attention !!! Ce produit est un import  si...   190093955   \n",
       "\n",
       "        imageid                               image name  image size in bits  \\\n",
       "101   933239989    image_933239989_product_190095382.jpg               43805   \n",
       "107   933233675    image_933233675_product_190091299.jpg               46184   \n",
       "339  1096608932  image_1096608932_product_1657064547.jpg               17070   \n",
       "346   955658863    image_955658863_product_220267350.jpg               56560   \n",
       "421   933235320    image_933235320_product_190093955.jpg               44208   \n",
       "\n",
       "     prdtypecode                              désignation textuelle  \\\n",
       "101           40                    Jeux vidéos anciens, équipement   \n",
       "107           40                    Jeux vidéos anciens, équipement   \n",
       "339         2585  Outillage intérieur / extérieur, tâches ménagères   \n",
       "346           40                    Jeux vidéos anciens, équipement   \n",
       "421           40                    Jeux vidéos anciens, équipement   \n",
       "\n",
       "          catégorie niv 1 desi_desc_cleaned  label  \n",
       "101           Jeux Vidéos               NaN     24  \n",
       "107           Jeux Vidéos               NaN     24  \n",
       "339  Maison & ameublement               NaN     21  \n",
       "346           Jeux Vidéos               NaN     24  \n",
       "421           Jeux Vidéos               NaN     24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveau DataFrame avec 750 lignes par catégorie :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\AppData\\Local\\Temp\\ipykernel_11228\\1826115207.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_cleaned.groupby('prdtypecode').apply(lambda x: x.sample(n=min(len(x), num_products_per_category), random_state=1))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>image name</th>\n",
       "      <th>image size in bits</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>désignation textuelle</th>\n",
       "      <th>catégorie niv 1</th>\n",
       "      <th>desi_desc_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resolving Stress In Your Marriage : How To Ide...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53291144</td>\n",
       "      <td>539067330</td>\n",
       "      <td>image_539067330_product_53291144.jpg</td>\n",
       "      <td>27972</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres anciens / occasion</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>resolving stress your marriage how identify an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Times Square To Timbuktu: The Post-Christ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196547987</td>\n",
       "      <td>946518577</td>\n",
       "      <td>image_946518577_product_196547987.jpg</td>\n",
       "      <td>10813</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres anciens / occasion</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>from times square timbuktu the post christian ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transnationalism In Iranian Political Thought</td>\n",
       "      <td>During the Iranian Revolution of 1978/9 the in...</td>\n",
       "      <td>1798131351</td>\n",
       "      <td>1151810891</td>\n",
       "      <td>image_1151810891_product_1798131351.jpg</td>\n",
       "      <td>31915</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres anciens / occasion</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>transnationalism iranian political thought dur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jeanne D'arc Opera En Quatre Actes Et Six Tabl...</td>\n",
       "      <td>Vendu en l'état - Exemplaire de travail.</td>\n",
       "      <td>390987066</td>\n",
       "      <td>1003160003</td>\n",
       "      <td>image_1003160003_product_390987066.jpg</td>\n",
       "      <td>34235</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres anciens / occasion</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>jeanne arc opera actes tableaux paroles musiqu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les Maladies De La Femmes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6408880</td>\n",
       "      <td>476582358</td>\n",
       "      <td>image_476582358_product_6408880.jpg</td>\n",
       "      <td>28749</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres anciens / occasion</td>\n",
       "      <td>Littérature</td>\n",
       "      <td>maladies femmes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Resolving Stress In Your Marriage : How To Ide...   \n",
       "1  From Times Square To Timbuktu: The Post-Christ...   \n",
       "2      Transnationalism In Iranian Political Thought   \n",
       "3  Jeanne D'arc Opera En Quatre Actes Et Six Tabl...   \n",
       "4                          Les Maladies De La Femmes   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN    53291144   539067330   \n",
       "1                                                NaN   196547987   946518577   \n",
       "2  During the Iranian Revolution of 1978/9 the in...  1798131351  1151810891   \n",
       "3           Vendu en l'état - Exemplaire de travail.   390987066  1003160003   \n",
       "4                                                NaN     6408880   476582358   \n",
       "\n",
       "                                image name  image size in bits  prdtypecode  \\\n",
       "0     image_539067330_product_53291144.jpg               27972           10   \n",
       "1    image_946518577_product_196547987.jpg               10813           10   \n",
       "2  image_1151810891_product_1798131351.jpg               31915           10   \n",
       "3   image_1003160003_product_390987066.jpg               34235           10   \n",
       "4      image_476582358_product_6408880.jpg               28749           10   \n",
       "\n",
       "       désignation textuelle catégorie niv 1  \\\n",
       "0  Livres anciens / occasion     Littérature   \n",
       "1  Livres anciens / occasion     Littérature   \n",
       "2  Livres anciens / occasion     Littérature   \n",
       "3  Livres anciens / occasion     Littérature   \n",
       "4  Livres anciens / occasion     Littérature   \n",
       "\n",
       "                                   desi_desc_cleaned  label  \n",
       "0  resolving stress your marriage how identify an...      0  \n",
       "1  from times square timbuktu the post christian ...      0  \n",
       "2  transnationalism iranian political thought dur...      0  \n",
       "3  jeanne arc opera actes tableaux paroles musiqu...      0  \n",
       "4                                    maladies femmes      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes par 'prdtypecode' :\n",
      " prdtypecode\n",
      "10      750\n",
      "40      750\n",
      "50      750\n",
      "60      750\n",
      "1140    750\n",
      "1160    750\n",
      "1180    750\n",
      "1280    750\n",
      "1281    750\n",
      "1300    750\n",
      "1301    750\n",
      "1302    750\n",
      "1320    750\n",
      "1560    750\n",
      "1920    750\n",
      "1940    750\n",
      "2060    750\n",
      "2220    750\n",
      "2280    750\n",
      "2403    750\n",
      "2462    750\n",
      "2522    750\n",
      "2582    750\n",
      "2583    750\n",
      "2585    750\n",
      "2705    750\n",
      "2905    750\n",
      "Name: count, dtype: int64\n",
      "Nombre de lignes par 'label' :\n",
      " label\n",
      "0     750\n",
      "24    750\n",
      "25    750\n",
      "26    750\n",
      "1     750\n",
      "2     750\n",
      "3     750\n",
      "4     750\n",
      "5     750\n",
      "6     750\n",
      "7     750\n",
      "8     750\n",
      "9     750\n",
      "10    750\n",
      "11    750\n",
      "12    750\n",
      "13    750\n",
      "14    750\n",
      "15    750\n",
      "16    750\n",
      "17    750\n",
      "18    750\n",
      "19    750\n",
      "20    750\n",
      "21    750\n",
      "22    750\n",
      "23    750\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# récupération du noim des classes\n",
    "class_names = sorted(os.listdir(DATA_DIR))\n",
    "print(\"class_names :\\n\", class_names)\n",
    "\n",
    "# mapping en index\n",
    "label_to_index = {label: idx for idx, label in enumerate(class_names)}\n",
    "print(\"label_to_index :\\n\", label_to_index)\n",
    "\n",
    "def map_label(code):\n",
    "    return label_to_index.get(str(code), None)\n",
    "\n",
    "df['label'] = df['prdtypecode'].apply(map_label).astype('Int64')\n",
    "display(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# identification des valeurs manquantes texte\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Valeurs manquantes par colonne :\\n\", missing_values)\n",
    "\n",
    "missing_desc = df[df['desi_desc_cleaned'].isnull()]\n",
    "print(\"Lignes avec des valeurs nulles dans 'desi_desc_cleaned' :\\n\")\n",
    "display(missing_desc.head())\n",
    "\n",
    "# suppression des lignes avec valeurs manquantes texte\n",
    "df_cleaned = df.dropna(subset = ['desi_desc_cleaned'])\n",
    "\n",
    "# constitution d'un dataframe avec 700 produits par catégorie\n",
    "num_products_per_category = 750\n",
    "df_sampled = df_cleaned.groupby('prdtypecode').apply(lambda x: x.sample(n=min(len(x), num_products_per_category), random_state=1))\n",
    "df_sampled = df_sampled.reset_index(drop=True)\n",
    "print(f\"Nouveau DataFrame avec {num_products_per_category} lignes par catégorie :\\n\")\n",
    "display(df_sampled.head())\n",
    "\n",
    "# Vérification du nombre de lignes par catégorie\n",
    "print(\"Nombre de lignes par 'prdtypecode' :\\n\", df_sampled['prdtypecode'].value_counts())\n",
    "print(\"Nombre de lignes par 'label' :\\n\", df_sampled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Création d'un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, dataframe, data_dir, base_transform=None, augment_transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.data_dir = data_dir\n",
    "        self.base_transform = base_transform\n",
    "        self.augment_transform = augment_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, str(self.dataframe.iloc[idx]['prdtypecode']), str(self.dataframe.iloc[idx]['image name']))\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        text = self.dataframe.iloc[idx]['desi_desc_cleaned']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        # Redimensionnement img\n",
    "        if self.base_transform:\n",
    "            image = self.base_transform(image)\n",
    "\n",
    "        # Augmentation img\n",
    "        if random.random() <= 0.3:  # 30% de probabilité\n",
    "            if self.augment_transform:\n",
    "                image = self.augment_transform(image)\n",
    "                \n",
    "        return image, text, label\n",
    "\n",
    "# Transformations de redimensionnement\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Transformations d'augmentation d'images\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_sampled, test_size = 0.2, stratify = df_sampled['label'], random_state = 42)\n",
    "\n",
    "train_dataset = ProductDataset(train_df, DATA_DIR, base_transform = base_transform, augment_transform = augment_transform)\n",
    "val_dataset = ProductDataset(val_df, DATA_DIR, base_transform = base_transform, augment_transform = augment_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classe pour la couche de classification initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'une couche de classification\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        outputs = self.clip_model(pixel_values=images, input_ids=texts)\n",
    "        \n",
    "        #log(f\"Shape of logits_per_image: {outputs.logits_per_image.shape}\")\n",
    "        if outputs.logits_per_image.shape[1] != self.classifier.in_features:\n",
    "            print(\"La dimension de logits_per_image ne correspond pas à la dimension d'entrée de la couche classifier.\")\n",
    "            print(f\"logits_per_image : {outputs.logits_per_image.shape[1]}\")\n",
    "            print(f\"classifier.in_features : {classifier.in_features}\")\n",
    "            \n",
    "        logits = self.classifier(outputs.logits_per_image)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instanciation du classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dégel de 1 couches sur 398 couches totales.\n",
      "La couche 398 est dégelée.\n",
      "La couche 399 est dégelée.\n",
      "La couche 400 est dégelée.\n",
      "3 couche(s) dégelée(s) sur un total de 400 couche(s).\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Gel des couches pré-entraînées\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Dégel de couches de sortie\n",
    "num_unfrozen_layers = 2\n",
    "unfrozen_count = 0\n",
    "total_layers = len(list(model.parameters()))\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx > (total_layers - num_unfrozen_layers):\n",
    "        param.requires_grad = True\n",
    "        unfrozen_count += 1\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "print(f\"Dégel de {unfrozen_count} couches sur {total_layers} couches totales.\")\n",
    "\n",
    "# Définition de la couche classifier\n",
    "classifier = Classifier(model, NUM_CLASSES)\n",
    "\n",
    "# Vérification\n",
    "unfrozen_count = 0\n",
    "total_params = list(classifier.parameters())\n",
    "for idx, param in enumerate(total_params):\n",
    "    if param.requires_grad:\n",
    "        unfrozen_count += 1\n",
    "        print(f\"La couche {idx + 1} est dégelée.\")\n",
    "print(f\"{unfrozen_count} couche(s) dégelée(s) sur un total de {len(total_params)} couche(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paramétres supplémentaires (critère d'évaluation, optimiseur, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = LR_REDUCER_PATIENCE, verbose = True)\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(classifier, optimizer, scheduler, epoch, val_loss, save_dir):\n",
    "\n",
    "    # Création du répertoire si inexistant\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # formatage du nom de fichier\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    save_path = os.path.join(save_dir, f'{current_time}_clip_epoch_{epoch}.pth')\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': classifier.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': val_loss,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "    }, save_path)\n",
    "\n",
    "    print(f\"Modèle sauvegardé à : {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sauvegarde des historiques loss et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history, save_dir, epoch):\n",
    "    \n",
    "    # Création du dossier de sauvegarde s'il n'existe pas\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "\n",
    "    # Création du path du fichier\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filepath = os.path.join(save_dir, f'{current_time}_clip_acc_loss_history_epoch{epoch}.pth')\n",
    "    \n",
    "    # Sauvegarde des listes dans un fichier .pkl\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'train_loss_history': train_loss_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_loss_history': val_loss_history,\n",
    "            'val_acc_history': val_acc_history\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Historique d'entraînement sauvegardé à : {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip_model(classifier,\n",
    "                     train_loader,\n",
    "                     val_loader,\n",
    "                     processor,\n",
    "                     criterion,\n",
    "                     optimizer,\n",
    "                     scheduler,\n",
    "                     epochs,\n",
    "                     save_dir,\n",
    "                     early_stopping_patience,\n",
    "                     train_loss_history = None,\n",
    "                     train_acc_history = None,\n",
    "                     val_loss_history = None,\n",
    "                     val_acc_history = None,\n",
    "                     start_epoch = 0,\n",
    "                     best_val_loss = float('inf')):\n",
    "    \n",
    "    # Initialisation des historiques si vides\n",
    "    if train_loss_history is None:\n",
    "        train_loss_history = []\n",
    "    if train_acc_history is None:\n",
    "        train_acc_history = []\n",
    "    if val_loss_history is None:\n",
    "        val_loss_history = []\n",
    "    if val_acc_history is None:\n",
    "        val_acc_history = []\n",
    "\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + epochs): \n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Entraînement\n",
    "        for images, texts, labels in tqdm(train_loader):\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True, do_rescale=False)\n",
    "            pixel_values = inputs['pixel_values']\n",
    "            input_ids = inputs['input_ids']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(pixel_values, input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, texts, labels in tqdm(val_loader):\n",
    "                inputs = processor(text = texts, images = images, return_tensors = \"pt\", padding = True, truncation = True, do_rescale = False)\n",
    "\n",
    "                pixel_values = inputs['pixel_values']\n",
    "                input_ids = inputs['input_ids']\n",
    "\n",
    "                outputs = classifier(pixel_values, input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Affichage des metrics et learning rate\n",
    "        print(f'Epoch [{epoch+1}/{start_epoch + epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        # Scheduler - Réduction du learning rate si nécessaire\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Sauvegarde du modèle\n",
    "        save_model(classifier, optimizer, scheduler, epoch+1, val_loss, save_dir)\n",
    "\n",
    "        # Sauvegarde des historiques accuracy et loss\n",
    "        save_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history, save_dir = ACC_LOSS_HIST_DIR, epoch = epoch + 1)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lancement du Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [46:14<00:00,  5.48s/it]\n",
      "100%|██████████| 126/126 [11:32<00:00,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 2.9023, Train Acc: 0.0937, Val Loss: 2.8505, Val Acc: 0.1064\n",
      "Learning Rate: 0.001000\n",
      "Modèle sauvegardé à : save\\2024-10-03_15-16-35_clip_epoch_1.pth\n",
      "Historique d'entraînement sauvegardé à : acc_loss_history\\2024-10-03_15-16-35_clip_acc_loss_history_epoch1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [46:10<00:00,  5.48s/it]\n",
      "100%|██████████| 126/126 [11:29<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Train Loss: 2.8369, Train Acc: 0.1030, Val Loss: 2.8390, Val Acc: 0.0925\n",
      "Learning Rate: 0.001000\n",
      "Modèle sauvegardé à : save\\2024-10-03_16-14-15_clip_epoch_2.pth\n",
      "Historique d'entraînement sauvegardé à : acc_loss_history\\2024-10-03_16-14-16_clip_acc_loss_history_epoch2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [46:13<00:00,  5.48s/it]\n",
      "100%|██████████| 126/126 [11:30<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Train Loss: 2.7938, Train Acc: 0.1096, Val Loss: 2.8026, Val Acc: 0.1163\n",
      "Learning Rate: 0.001000\n",
      "Modèle sauvegardé à : save\\2024-10-03_17-11-59_clip_epoch_3.pth\n",
      "Historique d'entraînement sauvegardé à : acc_loss_history\\2024-10-03_17-12-00_clip_acc_loss_history_epoch3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [46:24<00:00,  5.50s/it]\n",
      "100%|██████████| 126/126 [11:32<00:00,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Train Loss: 2.7651, Train Acc: 0.1104, Val Loss: 2.7495, Val Acc: 0.1168\n",
      "Learning Rate: 0.001000\n",
      "Modèle sauvegardé à : save\\2024-10-03_18-09-56_clip_epoch_4.pth\n",
      "Historique d'entraînement sauvegardé à : acc_loss_history\\2024-10-03_18-09-56_clip_acc_loss_history_epoch4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [46:13<00:00,  5.48s/it]\n",
      "100%|██████████| 126/126 [11:34<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Train Loss: 2.7439, Train Acc: 0.1167, Val Loss: 2.7713, Val Acc: 0.0965\n",
      "Learning Rate: 0.001000\n",
      "Modèle sauvegardé à : save\\2024-10-03_19-07-45_clip_epoch_5.pth\n",
      "Historique d'entraînement sauvegardé à : acc_loss_history\\2024-10-03_19-07-46_clip_acc_loss_history_epoch5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 61/506 [05:39<41:15,  5.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss_history, train_acc_history, val_loss_history, val_acc_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_clip_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEARLY_STOPPING_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 43\u001b[0m, in \u001b[0;36mtrain_clip_model\u001b[1;34m(classifier, train_loader, val_loader, processor, criterion, optimizer, scheduler, epochs, save_dir, early_stopping_patience, train_loss_history, train_acc_history, val_loss_history, val_acc_history, start_epoch, best_val_loss)\u001b[0m\n\u001b[0;32m     40\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 43\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[1;34m(self, images, texts)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, texts):\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#log(f\"Shape of logits_per_image: {outputs.logits_per_image.shape}\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlogits_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39min_features:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1304\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1299\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1300\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1301\u001b[0m )\n\u001b[0;32m   1302\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1304\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[0;32m   1312\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1313\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1318\u001b[0m )\n\u001b[0;32m   1320\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1043\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1040\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m   1041\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[1;32m-> 1043\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1050\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1051\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:824\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[1;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    816\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    817\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    818\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m         output_attentions,\n\u001b[0;32m    822\u001b[0m     )\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    831\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:559\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    556\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    558\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[1;32m--> 559\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    567\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:491\u001b[0m, in \u001b[0;36mCLIPSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    488\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# CLIP text model uses both `causal_attention_mask` and `attention_mask` sequentially.\u001b[39;00m\n\u001b[1;32m--> 491\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    501\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, embed_dim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_clip_model(\n",
    "    classifier = classifier, \n",
    "    train_loader = train_loader, \n",
    "    val_loader = val_loader, \n",
    "    processor = processor, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    scheduler = scheduler, \n",
    "    epochs = EPOCHS,\n",
    "    save_dir = 'save', \n",
    "    early_stopping_patience = EARLY_STOPPING_PATIENCE,\n",
    "    start_epoch = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classe pour réentraînement (à modifier) /!\\ \n",
    "- il n'y a pas d'attention mask dans la training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        outputs = self.clip_model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits_per_image\n",
    "        logits = self.classifier(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lancement du réentraînement (à modifier) /!\\ \n",
    "- charger optimizer, scheduler, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "classifier = Classifier(model, num_classes = NUM_CLASSES)\n",
    "\n",
    "# Avant le chargement des poids\n",
    "print(\"Poids de la couche classifier avant chargement :\")\n",
    "print(classifier.classifier.weight)\n",
    "\n",
    "classifier.load_state_dict(torch.load('save/2024-10-01_18-58-51_clip_epoch_30of30.pth'))\n",
    "\n",
    "# Après le chargement des poids\n",
    "print(\"Poids de la couche classifier après chargement :\")\n",
    "print(classifier.classifier.weight)\n",
    "\n",
    "print(classifier)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(classifier.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = LR_REDUCER_PATIENCE, verbose = True)\n",
    "\n",
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_clip_model(\n",
    "    classifier = classifier, \n",
    "    train_loader = train_loader, \n",
    "    val_loader = val_loader, \n",
    "    processor = processor, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    scheduler = scheduler, \n",
    "    epochs = EPOCHS,\n",
    "    save_dir = 'save', \n",
    "    early_stopping_patience = EARLY_STOPPING_PATIENCE,\n",
    "    start_epoch = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction de loading d'historique loss / acc (à vérifier) /!\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_history(save_dir, filename=\"training_history.pkl\"):\n",
    "    # Chemin complet pour le fichier de sauvegarde\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    \n",
    "    # Chargement des listes depuis le fichier .pkl\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            history = pickle.load(f)\n",
    "        print(f\"Historique d'entraînement chargé depuis {filepath}\")\n",
    "        return (history['train_loss_history'], \n",
    "                history['train_acc_history'], \n",
    "                history['val_loss_history'], \n",
    "                history['val_acc_history'])\n",
    "    else:\n",
    "        print(f\"Aucun fichier trouvé à {filepath}, les listes sont initialisées à vide.\")\n",
    "        return [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Appel de la fonction de chargement d'historique (à vérifier) /!\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = load_history(save_dir='history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history):\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize = (12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss_history, label = 'Train Loss', color = 'blue')\n",
    "    plt.plot(epochs, val_loss_history, label = 'Validation Loss', color = 'orange')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc_history, label = 'Train Accuracy', color = 'blue')\n",
    "    plt.plot(epochs, val_acc_history, label = 'Validation Accuracy', color = 'orange')\n",
    "    plt.title('CLIP Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rapport de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(y_true, y_pred, class_names):\n",
    "    report = classification_report(y_true, y_pred, target_names = class_names, output_dict = True)\n",
    "    df_report = pd.DataFrame(report).iloc[:-1, :].T\n",
    "\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    sns.heatmap(df_report.iloc[:-1, :].astype(float), annot = True, fmt = '.2f', cmap = 'Blues', cbar = True)\n",
    "    plt.title('CLIP Classification Report Heatmap')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Classes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues', xticklabels = class_names, yticklabels = class_names)\n",
    "    plt.title('CLIP Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation du modèle \n",
    "(A exécuter à la fin de la training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, val_loader, processor, class_names, train_loss_history, train_acc_history, val_loss_history, val_acc_history):\n",
    "    # Mise en mode évaluation\n",
    "    classifier.eval()\n",
    "\n",
    "    # Listes pour stocker les vrais labels et les prédictions\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Désactivation de la calcul des gradients pour l'évaluation\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in tqdm(val_loader):\n",
    "\n",
    "            # Prétraitement des données d'entrée\n",
    "            inputs = processor(text = texts, images = images, return_tensors = \"pt\", padding = True, truncation = True, do_rescale = False)\n",
    "            pixel_values = inputs['pixel_values']\n",
    "            input_ids = inputs['input_ids']\n",
    "\n",
    "            # Prédiction à partir du modèle\n",
    "            outputs = classifier(pixel_values, input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Stockage des résultats et conversion en array numpy\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Conversion des listes prédictions et true labels en tableaux numpy\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Visualisation des métriques\n",
    "    plot_training_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n",
    "    plot_classification_report(y_true, y_pred, class_names)\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom des classes\n",
    "class_names = df_sampled['label'].unique().tolist()\n",
    "\n",
    "# Appel de la fonction d'évaluation\n",
    "evaluate_model(classifier, val_loader, processor, class_names, train_loss_history, train_acc_history, val_loss_history, val_acc_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jul24-BDS-Rakuten kernel",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
